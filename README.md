# Efficient-MLLM-Insights

Efficient Multimodal Large Language Models: A Survey (August 2024)

Research Paper: [https://arxiv.org/pdf/2409.00743](https://arxiv.org/pdf/2405.10739)

Medium Article: [https://medium.com/@brandonjacklyn/interpretable-clustering-db306ccb9b1c](https://medium.com/@sameersah7365/revolutionising-ai-the-rise-of-efficient-multimodal-large-language-models-9e77ecb52cb7)

Slideshare Presentation: [https://www.slideshare.net/secret/LWdjAyMaOoDc2n](https://www.slideshare.net/slideshow/the-rise-of-efficient-multimodal-large-language-models-pdf/273820756)

# Abstract
Artificial intelligence has come a long way, but the most exciting breakthroughs are happening at the intersection of different types of data — text, images, videos, and sound. Imagine an AI system that can not only read a book but also analyze the illustrations and even understand spoken commentary about the book. This is the power of Multimodal Large Language Models (MLLMs).

MLLMs are transforming industries, excelling in tasks like visual question answering, captioning images, and interpreting complex data. However, there’s a catch: these models are enormous, computationally expensive, and difficult to deploy outside of high-resource environments. Their size and power requirements limit their accessibility, especially for smaller organizations and devices operating in constrained environments like smartphones or IoT devices.
